{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE888_HATE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lTXsMK3sNYr"
      },
      "source": [
        "!pip install transformers==2.8.0 #Installing the transformers library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juFykHwtl_8i"
      },
      "source": [
        "import os #Importing the OS module\n",
        "import random #Importing the random module\n",
        "import time #Importing the time module\n",
        "from tqdm import tqdm #To display progress bars \n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score #Importing the f1_score metric\n",
        "import requests #Import the requests module\n",
        "import torch #Importing the torch library\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
        "from transformers import BertTokenizer #Transformer import for tokenization\n",
        "from transformers import BertModel #Transformer import for the BertModel\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW #Optimization imports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4P0qKAiKLW"
      },
      "source": [
        "#Collecting the raw tweet data from github for the training,validation and testing files\n",
        "\n",
        "TRAIN_IN = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt'\n",
        "VAL_IN = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt'\n",
        "TEST_IN = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt'\n",
        "\n",
        "TRAIN_OUT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt'\n",
        "VAL_OUT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt'\n",
        "TEST_OUT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saAP35LC9tTT"
      },
      "source": [
        "#This function is used to convert and store the tweets and labels into their own dataframes\n",
        "#inputs represent the tweets and outputs represents the labels\n",
        "def txtdf(inputs, outputs):\n",
        "    post = []\n",
        "    targets = []\n",
        "    for sentence in inputs.split('\\n'):\n",
        "        post.append(sentence)\n",
        "    for target in outputs.split('\\n'):\n",
        "        try:\n",
        "            targets.append(int(target))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    dataframe= pd.DataFrame(post[:-1], columns=['text'])\n",
        "    dataframe['prediction'] = targets\n",
        "   \n",
        "    return dataframe\n",
        "\n",
        "#This function is used to retrieve the contents of the different datasets so that they can be converted to a dataframe\n",
        "\n",
        "def reqtxt (T_IN, T_OUT, V_IN, V_OUT, TE_IN, TE_OUT):\n",
        "  \n",
        "  ti = requests.get(T_IN).text\n",
        "  to = requests.get(T_OUT).text\n",
        "  tei = requests.get(TE_IN).text\n",
        "  teo = requests.get(TE_OUT).text\n",
        "  vi = requests.get(V_IN).text\n",
        "  vo = requests.get(V_OUT).text\n",
        "  training = txtdf(ti, to)\n",
        "  testing = txtdf(tei, teo)  \n",
        "  validation = txtdf(vi, vo)\n",
        "  \n",
        "  return training, testing, validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVKm2bR4pAwO"
      },
      "source": [
        "#Obtaining the dataframes for the training, validation and testing inputs / targets\n",
        "training, validation, testing = reqtxt(TRAIN_IN, TRAIN_OUT,VAL_IN, VAL_OUT,TEST_IN, TEST_OUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hxtI4l0SUJ"
      },
      "source": [
        "if torch.cuda.is_available():       #Checking for GPU availability\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDAfbCle59tP"
      },
      "source": [
        "#Calling the BERT Tokenizer\n",
        "token = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "\n",
        "def preprocessing_for_bert(data): #Function to perform the tokenization process\n",
        "\n",
        "    ids = [] #Used to hold the id output of the tokenization\n",
        "    attention_masks = [] #Used to hold the attention output of the tokenization\n",
        "\n",
        "    for sent in data:\n",
        "        encoded_sent = token.encode_plus(\n",
        "            text=sent, \n",
        "            add_special_tokens=True,   #Includes tokens for separations\n",
        "            max_length=MAX_LEN,        # The maximum length for truncation\n",
        "            pad_to_max_length=True,    # The length to which sentences will be padded\n",
        "            return_attention_mask=True) # To return the attention mask\n",
        "            \n",
        "        #Appending the outputs of the tokenization to the empty lists\n",
        "        ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Type conversion to tensor\n",
        "    ids = torch.tensor(ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTlQzTzAfCy7"
      },
      "source": [
        "MAX_LEN = 64 #Initializing the maximum length to 64\n",
        "#Retrieving the values for our training and validation inputs\n",
        "token_ids = list(preprocessing_for_bert([training.text[0]])[0].squeeze().numpy())\n",
        "X_train = training.text.values\n",
        "X_val = testing.text.values\n",
        "\n",
        "# Tokenization function used on the training and validation inputs to obtain the ids and attention masks\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuYEc61gcGL"
      },
      "source": [
        "#Converting the labels of the training and validation datasets to torch.Tensor\n",
        "train_labels = torch.tensor(training.prediction.values)\n",
        "val_labels = torch.tensor(testing.prediction.values)\n",
        "\n",
        "batch_size = 16 #Batch size set at 16 as it gives the best F1-Score\n",
        "\n",
        "# We create the dataloader for training here\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# We create the dataloader for validation here\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK41aBFSj5jK"
      },
      "source": [
        "# Creating a class for the Bert Classifier\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "        D_in, H, D_out = 768, 50, 2 #D_out is the number of targets we need for our dataset\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased') #Creating an instance of BERT\n",
        "        #Creating an instance of the feedforward classifier\n",
        "        self.classifier = nn.Sequential(nn.Linear(D_in, H),nn.ReLU(),nn.Linear(H, D_out))\n",
        "\n",
        "        if freeze_bert: #Freezing BERT\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "# Function to feed the tokens (IDs and Attention masks) to BERT\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :] #Using last cls for classification\n",
        "        logits = self.classifier(last_hidden_state_cls) #Computing logits\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX7su7Q_269U"
      },
      "source": [
        "#Function to initialize the BERT model\n",
        "def initialize_model(epochs=4):\n",
        "    bert_classifier = BertClassifier(freeze_bert=False) #Creating an instance of the BERT classifier\n",
        "    bert_classifier.to(device) #PyTorch to run the model on GPU if available\n",
        "\n",
        "    # Creating the optimizer\n",
        "    # Defining learning rate of the model \n",
        "    # Defining Epsilon value of the model \n",
        "    optimizer = AdamW(bert_classifier.parameters(),lr=5e-5,eps=1e-8)\n",
        "    \n",
        "    #No. of steps required for training\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Defining the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4HkhyECibW"
      },
      "source": [
        "#Defining the loss function for the model\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        " \n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "#Function used for training of the model\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        print(f\"{'Epoch':^6} | {'Batch':^6} | {'Train Loss':^10} | {'Val Loss':^8} | {'Val Acc':^6} | {'Elapsed':^6}\")\n",
        "        #Calculating time elapsed for each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Re-initialising the variables for next iteration\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Send batch to device's processor\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Setting gradients back to zero\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Feedfoward pass of the model\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Calculating the loss values and aggregating them\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backpropagation pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Updating of the hyperparameters as defined in optimizer\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 50 batches\n",
        "            if (step % 50 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                #Calculate the elapsed time for 50 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reseting the batch track variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Computing the average training loss \n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(\"****************************************\")\n",
        "\n",
        "        #Assessing the model's performance on the validation set\n",
        "        if evaluation == True:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Model was run successfully.\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "\n",
        "    model.eval() #model in evaluation mode\n",
        "\n",
        "    #Creating tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        # Sending the batch to the device's processor\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Computing the logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Computing the loss value of the model\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Obtaining the predictions of the model\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Obtaining the f1 score for each classification task\n",
        "        accuracy = f1_score(b_labels.tolist(), preds.tolist(), average = 'macro' )\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Computing overall performance of the BERT classifier\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfYw7dJ0U0v6"
      },
      "source": [
        "#Running the model\n",
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}